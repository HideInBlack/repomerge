{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仓库粒度的冲突消解-数据集预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.top4 数量的仓库冲突内容提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完毕，结果已写入到 XCoLab_Conflicts.txt 文件中。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 读取文件中的JSON数据\n",
    "input_path = \"G:/now/2024merge/mergeMinePython/json/50Repo/50RepoV1Pretty.json\"\n",
    "with open(input_path, 'r', encoding='utf-8') as file:\n",
    "    data_list = json.load(file)\n",
    "\n",
    "# 打开目标txt文件以写入内容\n",
    "with open('dataset/repoLevel/XCoLab_Conflicts.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for data in data_list:\n",
    "        if data.get('file_name') == \"./index_conflict_files/XCoLab/\":\n",
    "            a_contents = data.get('a_contents', '')\n",
    "            o_contents = data.get('o_contents', '')\n",
    "            b_contents = data.get('b_contents', '')\n",
    "            conflict = 'concurrent_change\\n' + a_contents + '\\nbase_content\\n' + o_contents + '\\nincoming_change\\n' + b_contents\n",
    "            processed_content = conflict.replace('\\n', ' _newline_ ').replace('\\t', ' ')\n",
    "            output_file.write(processed_content + '\\n')\n",
    "\n",
    "print(\"处理完毕，结果已写入到 XCoLab_Conflicts.txt 文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完毕，结果已写入到 accumulo.txt 文件中。\n"
     ]
    }
   ],
   "source": [
    "with open(input_path, 'r', encoding='utf-8') as file:\n",
    "    data_list = json.load(file)\n",
    "\n",
    "# 打开目标txt文件以写入内容\n",
    "with open('dataset/repoLevel/accumulo_Conflicts.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for data in data_list:\n",
    "        if data.get('file_name') == \"./index_conflict_files/accumulo/\":\n",
    "            a_contents = data.get('a_contents', '')\n",
    "            o_contents = data.get('o_contents', '')\n",
    "            b_contents = data.get('b_contents', '')\n",
    "            conflict = 'concurrent_change\\n' + a_contents + '\\nbase_content\\n' + o_contents + '\\nincoming_change\\n' + b_contents\n",
    "            processed_content = conflict.replace('\\n', ' _newline_ ').replace('\\t', ' ')\n",
    "            output_file.write(processed_content + '\\n')\n",
    "\n",
    "print(\"处理完毕，结果已写入到 accumulo.txt 文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完毕，结果已写入到 elasticsearch.txt 文件中。\n"
     ]
    }
   ],
   "source": [
    "with open(input_path, 'r', encoding='utf-8') as file:\n",
    "    data_list = json.load(file)\n",
    "\n",
    "# 打开目标txt文件以写入内容\n",
    "with open('dataset/repoLevel/elasticsearch_Conflicts.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for data in data_list:\n",
    "        if data.get('file_name') == \"./index_conflict_files/elasticsearch/\":\n",
    "            a_contents = data.get('a_contents', '')\n",
    "            o_contents = data.get('o_contents', '')\n",
    "            b_contents = data.get('b_contents', '')\n",
    "            conflict = 'concurrent_change\\n' + a_contents + '\\nbase_content\\n' + o_contents + '\\nincoming_change\\n' + b_contents\n",
    "            processed_content = conflict.replace('\\n', ' _newline_ ').replace('\\t', ' ')\n",
    "            output_file.write(processed_content + '\\n')\n",
    "\n",
    "print(\"处理完毕，结果已写入到 elasticsearch.txt 文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完毕，结果已写入到 OpenFaces.txt 文件中。\n"
     ]
    }
   ],
   "source": [
    "with open(input_path, 'r', encoding='utf-8') as file:\n",
    "    data_list = json.load(file)\n",
    "\n",
    "# 打开目标txt文件以写入内容\n",
    "with open('dataset/repoLevel/OpenFaces_Conflicts.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for data in data_list:\n",
    "        if data.get('file_name') == \"./index_conflict_files/OpenFaces/\":\n",
    "            a_contents = data.get('a_contents', '')\n",
    "            o_contents = data.get('o_contents', '')\n",
    "            b_contents = data.get('b_contents', '')\n",
    "            conflict = 'concurrent_change\\n' + a_contents + '\\nbase_content\\n' + o_contents + '\\nincoming_change\\n' + b_contents\n",
    "            processed_content = conflict.replace('\\n', ' _newline_ ').replace('\\t', ' ')\n",
    "            output_file.write(processed_content + '\\n')\n",
    "\n",
    "print(\"处理完毕，结果已写入到 OpenFaces.txt 文件中。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.代码块切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.方法签名 -> 信号量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing file: example.py\n",
      "Potential homologous functions:\n",
      "   Hash: f4f0b970f9afa6868a8670e1ccaf8229bcfcff7f57870d7e2383ce531b058898 Functions: ['add', 'sub', 'sum_all', 'sum_all_again']\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "# Define a normalized node type (using strings for simplicity)\n",
    "class NormalizedNode:\n",
    "    def __init__(self, kind, value=None):\n",
    "        self.kind = kind\n",
    "        self.value = value\n",
    "    def __repr__(self):\n",
    "      if self.value:\n",
    "          return f\"({self.kind}, {self.value})\"\n",
    "      else:\n",
    "          return f\"({self.kind})\"\n",
    "\n",
    "# Simple AST Normalizer class\n",
    "class AstNormalizer(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self.normalized_ast = []\n",
    "        self.variable_mapping = {}\n",
    "        self.variable_counter = 0\n",
    "\n",
    "    def process_ident(self, id_node):\n",
    "        # Change here: Get the name from the appropriate attribute\n",
    "        name = id_node.id if isinstance(id_node, ast.Name) else id_node.arg\n",
    "        if name in self.variable_mapping:\n",
    "            canonical_name = self.variable_mapping[name]\n",
    "            self.normalized_ast.append(NormalizedNode(\"ident\", canonical_name))\n",
    "        else:\n",
    "            canonical_name = f\"var{self.variable_counter}\"\n",
    "            self.variable_mapping[name] = canonical_name\n",
    "            self.normalized_ast.append(NormalizedNode(\"ident\", canonical_name))\n",
    "            self.variable_counter += 1\n",
    "\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "      self.normalized_ast.append(NormalizedNode(\"keyword\", \"function\"))\n",
    "      self.generic_visit(node)\n",
    "\n",
    "    def visit_Name(self, node):\n",
    "      self.process_ident(node)\n",
    "\n",
    "    def visit_arg(self, node):\n",
    "      self.process_ident(node)\n",
    "\n",
    "    def visit_stmt(self, node):\n",
    "       self.normalized_ast.append(NormalizedNode(\"statement\", ast.unparse(node)))\n",
    "       self.generic_visit(node)\n",
    "\n",
    "    def visit_expr(self, node):\n",
    "       self.normalized_ast.append(NormalizedNode(\"expression\", ast.unparse(node)))\n",
    "       self.generic_visit(node)\n",
    "\n",
    "    def visit_block(self, node):\n",
    "      self.normalized_ast.append(NormalizedNode(\"block\", ast.unparse(node)))\n",
    "      self.generic_visit(node)\n",
    "\n",
    "\n",
    "# Function to normalize an AST\n",
    "def normalize_ast(code):\n",
    "    tree = ast.parse(code)\n",
    "    normalizer = AstNormalizer()\n",
    "    normalizer.visit(tree)\n",
    "    return normalizer.normalized_ast\n",
    "\n",
    "\n",
    "# Simple hash function for a normalized AST\n",
    "def hash_normalized_ast(normalized_ast):\n",
    "    hashable_string = str(normalized_ast) # Convert the list of normalized nodes to a string for hashing\n",
    "    return hashlib.sha256(hashable_string.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def detect_homologous_functions(code):\n",
    "    tree = ast.parse(code)\n",
    "    function_hashes = {}\n",
    "\n",
    "    for item in tree.body:\n",
    "        if isinstance(item, ast.FunctionDef):\n",
    "            normalized_ast = normalize_ast(ast.unparse(item))\n",
    "            hash_value = hash_normalized_ast(normalized_ast)\n",
    "\n",
    "            if hash_value not in function_hashes:\n",
    "                function_hashes[hash_value] = []\n",
    "            function_hashes[hash_value].append(item.name)\n",
    "\n",
    "    print(\"Potential homologous functions:\")\n",
    "    for hash_val, functions in function_hashes.items():\n",
    "        if len(functions) > 1:\n",
    "            print(f\"   Hash: {hash_val} Functions: {functions}\")\n",
    "\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    if os.path.isdir(directory_path):\n",
    "        for entry in os.listdir(directory_path):\n",
    "            file_path = os.path.join(directory_path, entry)\n",
    "            if os.path.isfile(file_path):\n",
    "                print(f\"Analyzing file: {file_path}\")\n",
    "                with open(file_path, \"r\") as f:\n",
    "                  code = f.read()\n",
    "                  detect_homologous_functions(code)\n",
    "\n",
    "    elif os.path.isfile(directory_path):\n",
    "            print(f\"Analyzing file: {directory_path}\")\n",
    "            with open(directory_path, \"r\") as f:\n",
    "                code = f.read()\n",
    "                detect_homologous_functions(code)\n",
    "    else:\n",
    "        print(f\"Invalid file or directory: {directory_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an example.py file in the current directory for testing\n",
    "    example_code = \"\"\"\n",
    "def add(a, b):\n",
    "    sum = a + b\n",
    "    return sum\n",
    "\n",
    "def sub(c, d):\n",
    "    diff = c - d\n",
    "    return diff\n",
    "\n",
    "def sum_all(a, b):\n",
    "    result = a + b\n",
    "    return result\n",
    "\n",
    "def sum_all_again(x, y):\n",
    "    z = x + y\n",
    "    return z\n",
    "\"\"\"\n",
    "    with open(\"example.py\", \"w\") as f:\n",
    "        f.write(example_code)\n",
    "    process_directory(\"example.py\")\n",
    "    # To test on a directory containing several .py files, replace the path by the directory path in the process_directory method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
